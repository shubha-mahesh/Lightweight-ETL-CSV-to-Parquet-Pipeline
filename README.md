# Lightweight-ETL-CSV-to-Parquet-Pipeline
Built an end-to-end ETL pipeline to ingest large CSV datasets, perform schema validation &amp; cleaning, and write partitioned Parquet outputs (partitioned by `date` and `event_type`). Implemented idempotent incremental loads, transformation unit tests, and documentation for reproducible runs.  
